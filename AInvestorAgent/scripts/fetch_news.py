#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
scripts/fetch_news.py - ç»ˆæä¿®å¤ç‰ˆæœ¬
å…³é”®ä¿®å¤:
1. published_at æ—¥æœŸæ¯”è¾ƒä½¿ç”¨æ­£ç¡®çš„SQLå‡½æ•°
2. å»é‡åªæ£€æŸ¥æœ€è¿‘60å¤©
3. ä¿®å¤å­—æ®µé¡ºåº
"""

import sys
import os
import time
import argparse
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta, UTC

try:
    from dotenv import load_dotenv

    load_dotenv(dotenv_path=os.path.join(os.getcwd(), ".env"))
except Exception:
    pass

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import requests
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError
from sqlalchemy import text

from backend.storage.db import SessionLocal

try:
    from backend.storage.models import NewsRaw, NewsScore, Symbol

    HAS_SYMBOL_MODEL = True
except Exception:
    from backend.storage.models import NewsRaw, NewsScore

    HAS_SYMBOL_MODEL = False

NEWS_API_URL = os.getenv("NEWS_API_URL", "https://newsapi.org/v2/everything")
NEWS_API_KEY = os.getenv("NEWS_API_KEY", "")
DEFAULT_TIMEOUT = int(os.getenv("NEWS_TIMEOUT", "35"))


def _to_iso_utc_days_ago(days: int) -> str:
    dt = datetime.now(UTC) - timedelta(days=int(days))
    return dt.strftime("%Y-%m-%dT%H:%M:%SZ")


def _normalize_published_at(raw: Optional[str]):
    """
    æ ‡å‡†åŒ–æ—¶é—´ä¸º Python datetime å¯¹è±¡
    SQLAlchemyçš„DATETIMEç±»å‹è¦æ±‚datetimeå¯¹è±¡ï¼Œä¸èƒ½æ˜¯å­—ç¬¦ä¸²ï¼
    """
    if not raw:
        return None
    x = raw.strip()
    if not x:
        return None

    # è§£æISOæ ¼å¼: 2025-10-24T16:26:25Z
    # ç§»é™¤æ—¶åŒºæ ‡è®° 'Z'ï¼Œè½¬æ¢ä¸ºdatetimeå¯¹è±¡
    try:
        # ç§»é™¤ 'Z' åç¼€
        if x.endswith('Z'):
            x = x[:-1]

        # è§£æä¸ºdatetimeå¯¹è±¡
        from datetime import datetime as dt
        return dt.fromisoformat(x)
    except Exception:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›None
        return None


def _get_company_name(db: Session, symbol: str) -> Optional[str]:
    if HAS_SYMBOL_MODEL:
        row = db.query(Symbol).filter(Symbol.symbol == symbol).first()
        return getattr(row, "name", None) if row else None
    try:
        result = db.execute(text("SELECT name FROM symbols WHERE symbol = :sym LIMIT 1"), {"sym": symbol}).fetchone()
        return result[0] if result and result[0] else None
    except Exception:
        return None


def _count_scored(db: Session, ids: List[int]) -> int:
    if not ids:
        return 0
    placeholders = ",".join([f":p{i}" for i in range(len(ids))])
    q = f"SELECT COUNT(1) FROM news_scores WHERE news_id IN ({placeholders})"
    params = {f"p{i}": nid for i, nid in enumerate(ids)}
    row = db.execute(text(q), params).fetchone()
    return int(row[0]) if row else 0


def fetch_news(symbol: str, *, days: int = 30, pages: int = 1,
               timeout: int = DEFAULT_TIMEOUT, noproxy: bool = False,
               db_for_query: Optional[Session] = None,
               debug: bool = False) -> List[Dict[str, Any]]:
    if not NEWS_API_KEY:
        raise RuntimeError("ç¼ºå°‘ NEWS_API_KEY ç¯å¢ƒå˜é‡")

    company = None
    if db_for_query is not None:
        try:
            company = _get_company_name(db_for_query, symbol)
        except Exception:
            company = None

    query = f'"{company}" OR {symbol}' if company else symbol
    if debug:
        print(f"  ğŸ” æœç´¢è¯: {query}")

    session = requests.Session()
    if noproxy:
        session.trust_env = False
    headers = {"X-Api-Key": NEWS_API_KEY, "Accept": "application/json", "Connection": "close"}

    since_iso = _to_iso_utc_days_ago(days)
    all_articles: List[Dict[str, Any]] = []

    for p in range(1, int(pages) + 1):
        params = {
            "q": query,
            "language": "en",
            "from": since_iso,
            "sortBy": "publishedAt",
            "pageSize": 100,
            "page": p,
        }

        if debug:
            print(f"  ğŸ“¡ è¯·æ±‚ç¬¬{p}é¡µ...")

        try:
            r = session.get(NEWS_API_URL, params=params, headers=headers, timeout=timeout)
        except requests.RequestException as e:
            sys.stderr.write(f"[warn] {symbol} p{p} request error: {e}\n")
            break

        if r.status_code != 200:
            sys.stderr.write(f"[warn] {symbol} p{p} HTTP {r.status_code}: {r.text[:160]}\n")
            break

        data = r.json() or {}
        articles = data.get("articles") or []
        total_results = data.get("totalResults", 0)

        if debug:
            print(f"  ğŸ“° APIè¿”å›: {len(articles)}æ¡ (æ€»å…±{total_results}æ¡)")

        if not articles:
            break

        all_articles.extend(articles)

        if len(articles) < 100:
            break

        time.sleep(0.6)

    if debug:
        print(f"  âœ… å…±è·å– {len(all_articles)} æ¡æ–°é—»")

    return all_articles


def upsert_news_and_score(db: Session, symbol: str, items: List[Dict[str, Any]], *,
                          rescore_window_days: int = 30,
                          debug: bool = False) -> Dict[str, int]:
    """
    å°†æ–°é—»å…¥åº“å¹¶æ‰“åˆ†

    ğŸ”§ å…³é”®ä¿®å¤: ä½¿ç”¨ SQL datetime() å‡½æ•°æ­£ç¡®æ¯”è¾ƒæ—¥æœŸ
    """
    inserted = dupe = 0

    # ğŸ”§ ä¿®å¤: ä½¿ç”¨SQLçš„datetimeå‡½æ•°æ¥æ­£ç¡®æ¯”è¾ƒæ—¥æœŸ
    # è®¡ç®—60å¤©å‰çš„æ—¥æœŸ
    cutoff_days = 60

    # ä½¿ç”¨åŸç”ŸSQLï¼Œè®©SQLiteå¤„ç†æ—¥æœŸæ¯”è¾ƒ
    existing_urls_query = text("""
        SELECT url FROM news_raw 
        WHERE symbol = :symbol 
        AND datetime(published_at) >= datetime('now', :delta)
    """)

    result = db.execute(existing_urls_query, {
        "symbol": symbol,
        "delta": f"-{cutoff_days} days"
    })

    existing_urls = {row[0] for row in result if row[0]}

    if debug:
        print(f"  ğŸ“¦ æ•°æ®åº“ä¸­æœ€è¿‘{cutoff_days}å¤©å·²æœ‰ {len(existing_urls)} æ¡URL")

    batch_urls = set()

    for i, a in enumerate(items, 1):
        url = (a.get("url") or "").strip()
        if not url:
            continue

        if url in batch_urls:
            dupe += 1
            continue

        if url in existing_urls:
            dupe += 1
            if debug and i <= 3:
                print(f"    âš ï¸ URLå·²å­˜åœ¨: {url[:60]}...")
            continue

        # ğŸ”§ ä¿®å¤: publishedAt åœ¨å‰
        pub_raw = a.get("publishedAt") or a.get("published_at") or ""
        published_at = _normalize_published_at(pub_raw)

        src_obj = a.get("source") or {}
        source = (src_obj.get("name") if isinstance(src_obj, dict) else str(src_obj)) or "unknown"

        obj = NewsRaw(
            symbol=symbol,
            title=a.get("title") or "",
            summary=a.get("description") or "",
            url=url,
            source=source,
            published_at=published_at
        )

        try:
            db.add(obj)
            db.flush()
            batch_urls.add(url)
            inserted += 1

            if debug and inserted <= 5:
                print(f"    âœ… æ–°å¢: {published_at} - {obj.title[:40]}...")

        except IntegrityError:
            db.rollback()
            dupe += 1
            if debug and dupe <= 3:
                print(f"    âš ï¸ é‡å¤(IntegrityError): {url[:60]}...")
        except Exception as e:
            db.rollback()
            if debug:
                print(f"    âŒ å…¥åº“å¤±è´¥: {e}")
            continue

    try:
        db.commit()
        if debug:
            print(f"  ğŸ’¾ æäº¤å®Œæˆ: æ–°å¢{inserted}, å»é‡{dupe}")
    except IntegrityError:
        db.rollback()
        if debug:
            print(f"  âŒ æäº¤å¤±è´¥(IntegrityError)")

    # æ‰“åˆ†é˜¶æ®µ - å¤„ç†æ‰€æœ‰æœªæ‰“åˆ†çš„æ–°é—»ï¼ˆåŒ…æ‹¬ä¹‹å‰å…¥åº“ä½†æœªæ‰“åˆ†çš„ï¼‰
    scored = 0
    try:
        from backend.sentiment.scorer import classify_polarity
        from datetime import datetime as dt

        # æŸ¥æ‰¾è¯¥è‚¡ç¥¨æ‰€æœ‰æœªæ‰“åˆ†çš„æ–°é—»ï¼ˆä¸é™äºåˆšæ’å…¥çš„ï¼‰
        window_sql = text("""
            SELECT nr.id
            FROM news_raw AS nr
            LEFT JOIN news_scores AS ns ON ns.news_id = nr.id
            WHERE nr.symbol = :sym
              AND datetime(nr.published_at) >= datetime('now', :delta)
              AND ns.news_id IS NULL
            ORDER BY nr.id DESC
            LIMIT 1000
        """)

        delta = f"-{int(rescore_window_days)} days"
        ids = [row[0] for row in db.execute(window_sql, {"sym": symbol, "delta": delta}).fetchall()]

        if ids:
            if debug:
                print(f"  ğŸ§® å¾…æ‰“åˆ†æ–°é—»: {len(ids)}æ¡ï¼ˆåŒ…æ‹¬å†å²æœªæ‰“åˆ†ï¼‰")

            # è·å–æ–°é—»è®°å½•
            news_items = db.query(NewsRaw).filter(NewsRaw.id.in_(ids)).all()

            # é€æ¡æ‰“åˆ†
            for news in news_items:
                # ä½¿ç”¨classify_polarityè®¡ç®—æƒ…ç»ª
                sentiment = classify_polarity(news.title or "", news.summary or "")

                # åˆ›å»ºæ‰“åˆ†è®°å½•
                score_record = NewsScore(
                    news_id=news.id,
                    sentiment=sentiment
                )
                db.add(score_record)
                scored += 1

            db.commit()

            if debug:
                print(f"  âœ… å®Œæˆæ‰“åˆ†: {scored}æ¡")
        else:
            if debug:
                print(f"  â„¹ï¸ æ— éœ€æ‰“åˆ†çš„æ–°é—»")

    except Exception as e:
        db.rollback()
        if debug:
            print(f"  âš ï¸ æ‰“åˆ†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    return {"inserted": inserted, "dupe": dupe, "scored": scored}


def main():
    p = argparse.ArgumentParser(description="æ‰¹é‡æŠ“å–æ–°é—» â†’ å…¥åº“ â†’ æ‰“åˆ†")
    p.add_argument("--symbols", type=str, required=True, help="è‚¡ç¥¨åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”")
    p.add_argument("--days", type=int, default=30, help="æŠ“å–æœ€è¿‘Nå¤©çš„æ–°é—»")
    p.add_argument("--pages", type=int, default=2, help="æ¯ä¸ªè‚¡ç¥¨æŠ“å–Né¡µ")
    p.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT)
    p.add_argument("--noproxy", action="store_true")
    p.add_argument("--rescore-window-days", type=int, default=30)
    p.add_argument("--debug", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯")
    args = p.parse_args()

    symbols = [s.strip().upper() for s in args.symbols.split(",") if s.strip()]
    if not symbols:
        print("âš ï¸ æœªæä¾›æœ‰æ•ˆçš„ symbols")
        sys.exit(1)
    if not NEWS_API_KEY:
        print("âš ï¸ ç¼ºå°‘ NEWS_API_KEY ç¯å¢ƒå˜é‡")
        sys.exit(2)

    total = {"inserted": 0, "dupe": 0, "scored": 0}

    print(f"\n{'=' * 70}")
    print(f"å¼€å§‹å¤„ç† {len(symbols)} åªè‚¡ç¥¨")
    print(f"{'=' * 70}\n")

    with SessionLocal() as db:
        for i, sym in enumerate(symbols, 1):
            print(f"[{i}/{len(symbols)}] å¤„ç† {sym}...")

            try:
                items = fetch_news(
                    sym,
                    days=args.days,
                    pages=args.pages,
                    timeout=args.timeout,
                    noproxy=args.noproxy,
                    db_for_query=db,
                    debug=args.debug
                )

                stats = upsert_news_and_score(
                    db,
                    sym,
                    items,
                    rescore_window_days=args.rescore_window_days,
                    debug=args.debug
                )

                for k in total:
                    total[k] += stats.get(k, 0)

                print(f"âœ… {sym}: æ–°å¢ {stats['inserted']} æ¡, å»é‡ {stats['dupe']} æ¡, æ‰“åˆ† {stats['scored']} æ¡\n")

            except Exception as e:
                print(f"âš ï¸ {sym} æŠ“å–å¤±è´¥: {e}\n")
                if args.debug:
                    import traceback
                    traceback.print_exc()

    print(f"{'=' * 70}")
    print(f"âœ… å®Œæˆï¼šæ–°å¢={total['inserted']} å»é‡={total['dupe']} æ‰“åˆ†={total['scored']}")
    print(f"{'=' * 70}")


if __name__ == "__main__":
    main()